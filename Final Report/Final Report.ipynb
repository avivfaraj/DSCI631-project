{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Rk_Vd_aKzv5"
   },
   "source": [
    "# Final Report\n",
    "<h1> New York City Property Sales </h1>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEVPh9rOLACk"
   },
   "source": [
    "## Team Members\n",
    "\n",
    "* **Aviv Farag** - af3228@drexel.edu\n",
    "\n",
    "* **Jospeh Logan** - jrl362@drexel.edu\n",
    "\n",
    "* **Abdulaziz Alquzi** - aa4472@drexel.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhJyozbALKhb"
   },
   "source": [
    "---\n",
    "## Discussion \n",
    "\n",
    "We are data science consultants who are contracted by property management investors in New York City. Their company, supported by investors, wants to buy residential real estate in NYC at the cheapest price possible, renovate, then resell within a year. The renovation analysis is outside the scope of this project, but they want a baseline model that can predict the price of residential real-estate in order to :\n",
    " \n",
    "1. Identify potential undervalued listed properties to buy\n",
    "2. Predict market price when it’s time to sell in order to sell quickly while maximizing return on investment\n",
    " \n",
    "Because the want to renovate and sell the properties quickly, they want less than 10 residential units, and properties less than 5 million each but are at least ten thousand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDoJ_FbeAAOT"
   },
   "source": [
    "---\n",
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efWBIsCsLOUY"
   },
   "source": [
    "### Columns Description \n",
    "\n",
    "1. **Borough** is the borough in which the unit is located: '1':'Manhattan', '2':'Bronx', '3': 'Brooklyn', '4':'Queens','5':'Staten Island'. Location is a key feature in real estate, and this is especially true in NYC. For the purposes of exploratory analysis, we have converted the numeric values into their proper names.  Below you will see a clear distinction in price by neighborhood, with Manhattan being much more expensive. For constructing the model we will likely one hot encode.\n",
    " \n",
    "1. **Neighborhood** is the specific neighborhood within the borough. There is a strong relationship with neighborhood and sale price. Much like borough, because the neighborhoods are not explicitly ranked, a one hot coding strategy will likely be used\n",
    " \n",
    "1. **Building Class Category** is an important feature as it separates between attached and unattached houses or elevator apartments\n",
    " \n",
    "1. **Tax class a present** is the tax class and is heavily correlated with both sale price and tax class at time of sale. Tax class and number of units as correlated as the tax class depends on how may units. There is a big risk of data leakage with this feature. The models real world success would depend on accurately determining the tax class before selling or purchasing a property. Because of this, we may want to remove this feature.\n",
    "[Source](https://blocksandlots.com/property-taxes-in-nyc-all-you-need-to-know/)\n",
    "\n",
    "1. **Block and Lot** The combination of borough, block, and lot forms a unique key for property in New York City. Commonly called a BBL\n",
    "[Source](https://blocksandlots.com/property-taxes-in-nyc-all-you-need-to-know/)\n",
    " \n",
    "1. **Building Class at present** <br><br>\n",
    "According to [nyc.gov](https://www1.nyc.gov/assets/finance/downloads/pdf/07pdf/glossary_rsf071607.pdf): <br>\n",
    "“The Building Classification is used to describe a property’s constructive use. The first position of the Building Class is a letter that is used to describe a general class of properties (for example “A” signifies one-family homes, “O” signifies office buildings. “R” signifies condominiums). The second position, a number, adds more specific information about the property’s use or construction style (using our previous examples “A0” is a Cape Cod style one family home, “O4” is a tower type office building and “R5” is a commercial condominium unit). The term Building Class used by the Department of Finance is interchangeable with the term Building Code used by the Department of Buildings.”\n",
    "<br>\n",
    "Because this feature has direct overlap with many other features, we will likely remove it.\n",
    "\n",
    "1. **Address** Is the actual address. Because of the variance we will remove this feature. However, it could be potentially used to crosswalk longitude and latitude, but that would require an additional dataset.\n",
    " \n",
    "1. **Zip Code** Zip codes are difficult to work with in machine learning problems because it’s an integer and a higher or lower zip code won’t necessarily mean it’s better or worse. If we are going to use it we will one hot encode\n",
    " \n",
    "1. **Residential Units** are the number of residential units for sale. This is correlated with price as 8 units will likely cost more than 2 in a similar neighborhood. For exploratory analysis we examine the price per unit.\n",
    " \n",
    "1. **Land Square Feet** is the land area of the property. This is a valuable feature for predicting price\n",
    " \n",
    "1. **Gross Square Feet** According to nyc.gov: “The total area of all the floors of a building as measured from the exterior surfaces of the outside walls of the building, including the land area and space within any building or structure on the property.”  This is also a valuable feature for predicting price. However, it’s important for compare between this and location as a smaller property in the center of Manhattan, may be more expensive than a much larger property in Staten Island.\n",
    " \n",
    "1. **Year Built** is the year the structure was built. Many of the properties were built a long time ago, but it’s worth further testing this feature before elimination. \n",
    "Tax class at time of sale: See above for tax class. It will be difficult to accurately predict this and it has a very high risk of data leakage. This feature will almost certainly be removed.\n",
    "\n",
    "1. **Tax Class at Time of Sale** - <br> \n",
    "  &nbsp;[Property Tax Rate](https://www1.nyc.gov/site/finance/taxes/property-tax-rates.page \"Click here to move to NYC property tax rates\") for 2016/2017 was:\n",
    "    - Class 1 - 19.991\\%\t\n",
    "    - Class 2 - 12.892\\%\t\n",
    "    - Class 3 - 10.934\\%\t\n",
    "    - Class 4 - 10.574\\% <br><br>\n",
    "        **Note**:This feature might cause data leakage due to its correlation with SALE PRICE (higher price -> higher tax), and therefore we will not use it in our machine learning algorithm.\n",
    "\n",
    "1. **Sale Price** is our target variable. Due to the scope of the business problem we are limiting the dataset to 10,000 and 5,000,0000\n",
    " \n",
    "1. **Sale Date** Is the date of the sale. We may want to look at the sale month to determine if we can purchase a property in a slower month for real estate i.e. buy in the winter cheaply and resell in a hotter month like this the spring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb9W9z33_7ZG"
   },
   "source": [
    "### Residential Units vs. Sale Price\n",
    "\n",
    "We used Seaborn package to plot a boxplot of residential units versus sale price. \n",
    "\n",
    "```\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='RESIDENTIAL UNITS', y='SALE PRICE', data=df)\n",
    "plt.title('Residential Units vs Sale Price')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<center><img  src=\"./img/res_vs_saleprice.png\" width = \"60%\" />\n",
    "<br>\n",
    "<b><u>Fig. 1</u></b>: Box plot of Residential Units and Sale Price </center>\n",
    "\n",
    "The mean SALE PRICE of 8 residential units is higher than the mean value of any other number of residential units. Also, mean SALE PRICE increases as the residential units increase up to 8 residential units. Then, it goes down for 9 residential units. Moreover, there are a lot of outliers in 1,2,3, and 4 residential units which could be due to Borough or other parameters that were not taken into account in this plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0xN5hxSAETX"
   },
   "source": [
    "### Borough vs. Sale Price\n",
    "\n",
    "```\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.boxplot(x='BOROUGH', y='SALE PRICE', data=df)\n",
    "plt.title('Borough vs Sale Price')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "<center><img  src=\"./img/borough_vs_saleprice.png\" width = \"60%\" />\n",
    "<br>\n",
    "<b><u>Fig. 2</u></b>: Borough vs Sale Price </center>\n",
    "\n",
    "The figure above presents the distribution of Sale price among Borough. The mean SALE PRICE in Manhattan is the highest. Also, the distribution of SALE PRICE in Manhattan is the longest (up to ~4.5 million without outliers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xfQh6fSdKiY"
   },
   "source": [
    "### Correlation Table\n",
    "\n",
    "```\n",
    "# New copy of dataframe\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Define costum label encoder (inline lambda function)\n",
    "label_encoder = lambda x: {uniq : i for i,uniq in enumerate(df[x].unique().tolist())}\n",
    "\n",
    "df_copy[\"SALE PRICE\"] = np.log(df_copy[\"SALE PRICE\"])\n",
    "\n",
    "# Map different columns to numbers\n",
    "df_copy['NEIGHBORHOOD'] = df_copy['NEIGHBORHOOD'].map(label_encoder('NEIGHBORHOOD'))\n",
    "df_copy['BUILDING CLASS CATEGORY'] = df_copy['BUILDING CLASS CATEGORY'].map(label_encoder('BUILDING CLASS CATEGORY'))\n",
    "df_copy[\"TAX CLASS AT PRESENT\"] = df_copy[\"TAX CLASS AT PRESENT\"].map({'1': 1,'1A':1, '1B':1, '1C':1, \n",
    "                                                             '2':2, '2C':2, '2B':2, '2A':2,\n",
    "                                                             '4':4})\n",
    "```\n",
    "We made a copy of the DataFrame and defined a costumed label encoder using lambda function. Additionally, we used logarithm on the target variable due to large amount of outliers. Then, we converted string values to numbers for some features using our label encoder function. The correlation results are presented in the figure below:\n",
    "\n",
    "\n",
    "<center><img  src=\"./img/correlation_table.png\" width = \"80%\" />\n",
    "<br>\n",
    "<b><u>Fig. 3</u></b>: Correlation table </center>\n",
    "\n",
    "\n",
    "Based on this correlation table and the other figures above we chose a few features for our machine learning model:\n",
    "1. Borough -> values 1 to 5\n",
    "2. Neighborhood -> using label encoder to transform strings to numbers\n",
    "3. Block -> numerical \n",
    "4. Lot -> numerical\n",
    "5. Building Class Category -> Strings (OneHotEncoder because number of attributes is low)\n",
    "6. Residential Units -> values 1 to 9\n",
    "7. Gross Square Feet -> numerical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_5WQtU0LQ-c"
   },
   "source": [
    "---\n",
    "## Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RULFQ-DtLezP"
   },
   "source": [
    "### Models\n",
    "We implemented Linear Regression, Decision Tree Regressor, Extre Tree Regressor and Random Forest Regressor in order to predict property price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ayMCAvyjiui"
   },
   "source": [
    "### Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0ZkdUObkcDu"
   },
   "source": [
    "#### Label Encoder Function\n",
    "\n",
    "Defined label encoder function to ease the process of converting unique values (strings) to numbers:\n",
    "```\n",
    "# Define costum label encoder (inline lambda function)\n",
    "label_encoder = lambda x: {uniq : i for i,uniq in enumerate(df[x].unique().tolist())}\n",
    "\n",
    "def label_encode(column = \"\"):\n",
    "    if column:\n",
    "        df[column] = df[column].map(label_encoder(column))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er46HfMFkGaM"
   },
   "source": [
    "#### Tree Regressors\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Label Encoder for neighborhood and Building class categroy\n",
    "label_encode(\"NEIGHBORHOOD\")\n",
    "label_encode(\"BUILDING CLASS CATEGORY\")\n",
    "\n",
    "# DataFrame for Linear Regression (remove outliers)\n",
    "lr_df = df.copy()\n",
    "\n",
    "# Log sale price\n",
    "df[\"SALE PRICE\"] = np.log(df[\"SALE PRICE\"])\n",
    "\n",
    "# Chosen features for ML models\n",
    "modified_df = df[[\"BOROUGH\",\n",
    "                  \"NEIGHBORHOOD\",\n",
    "                  \"BLOCK\",\n",
    "                  \"LOT\",\n",
    "                  \"BUILDING CLASS CATEGORY\",\n",
    "                  \"RESIDENTIAL UNITS\",\n",
    "                  \"GROSS SQUARE FEET\",\n",
    "                  ]]\n",
    "\n",
    "# Target column\n",
    "y = df[\"SALE PRICE\"]\n",
    "\n",
    "# Split to train and test\n",
    "tr_x_train, tr_x_test, tr_y_train, tr_y_test = train_test_split(modified_df,y, test_size=0.2, random_state=42)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LaFHky6kNIe"
   },
   "source": [
    "#### Linear Regression\n",
    "In order to deal with outliers in an efficient way and without removing too many instances from our dataset, we trained the linear regression model on price per unit in a logarithm scale. In other words, we divided the SALE PRICE column with RESIDENTIAL UNITS and the results we transformed to a logarithm scale. In the validation section, we would like to convert the predictions back to the original SALE PRICE's domain, and therefore we initalized two variables: lr_y_test and lr_y_test_ru. The first one (lr_y_test) is SALE PRICE which we would like to predict and the second one (lr_y_test_ru) is the residential units of the target column which we need in order to convert price per unit back to SALE PRICE. \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# Handling outliers for Linear Regression\n",
    "# Calculate price per unit and transform to log\n",
    "lr_df[\"price per unit\"] = np.log(lr_df[\"SALE PRICE\"] / lr_df[\"RESIDENTIAL UNITS\"])\n",
    "\n",
    "# Remove outliers \n",
    "lr_df = lr_df[(lr_df[\"price per unit\"] > 11.7) & (lr_df[\"price per unit\"] < 14.8)]\n",
    "\n",
    "lr_modified = lr_df[[\"BOROUGH\",\n",
    "                  \"NEIGHBORHOOD\",\n",
    "                  \"BLOCK\",\n",
    "                  \"LOT\",\n",
    "                  \"BUILDING CLASS CATEGORY\",\n",
    "                  \"RESIDENTIAL UNITS\",\n",
    "                  \"GROSS SQUARE FEET\",\n",
    "                  \"price per unit\",\n",
    "                  \"SALE PRICE\"]]\n",
    "\n",
    "# Split to train and test\n",
    "train_set, test_set = train_test_split(lr_modified, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split data and target in training set\n",
    "lr_y_train_ppu = train_set[\"price per unit\"].copy()\n",
    "lr_x_train_ppu = train_set.drop([\"SALE PRICE\",\"price per unit\",\"RESIDENTIAL UNITS\"], axis = 1)\n",
    "\n",
    "# Split data and target in test set\n",
    "lr_y_test_ppu = test_set[\"price per unit\"].copy()\n",
    "lr_y_test = test_set[\"SALE PRICE\"].copy()\n",
    "lr_y_test_ru = test_set[\"RESIDENTIAL UNITS\"]\n",
    "lr_x_test_ppu = test_set.drop([\"SALE PRICE\",\"price per unit\",\"RESIDENTIAL UNITS\"], axis = 1)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeiEVxkOaVFe"
   },
   "source": [
    "### Baseline - Dummy Regressor\n",
    "There are two dummy regressors, one for Linear Regression and the other one for the other regressors. The reason for having two baselines is that the model for linear regression is different because Linear Regression is sensitive to outliers, and therefore the model is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKxXkEBchO4s"
   },
   "source": [
    "#### Trees Baseline \n",
    "```\n",
    "print(\"Trees df\")\n",
    "\n",
    "# Initialize Dummy estimator \n",
    "dummy_regr = DummyRegressor(strategy=\"median\")\n",
    "\n",
    "# Train estimator\n",
    "dummy_regr.fit(tr_x_train, tr_y_train)\n",
    "\n",
    "# Predict \n",
    "preds = np.exp(dummy_regr.predict(tr_x_test))\n",
    "\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(np.exp(tr_y_test), preds)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(np.exp(tr_y_test), preds)\n",
    "\n",
    "# Print results\n",
    "print(\"R-Squared: \", r2_score(np.exp(tr_y_test), preds))\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "print(\"Mean Absolute Error: \", mae)\n",
    "print()\n",
    "```\n",
    "results:\n",
    "\n",
    "\n",
    "```\n",
    "Trees df\n",
    "R-Squared: -0.107\n",
    "Root Mean Square Error: 602,116.561\n",
    "Mean Absolute Error: 346,494.955\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VWPRh9thFrt"
   },
   "source": [
    "\n",
    "#### Linear Regression Baseline\n",
    "As mentioned earlier, we used a different model for Linear Regression due to it sensitivity to outliers. \n",
    "```\n",
    "print(\"Linear Regression df\")\n",
    "\n",
    "# Initialize Dummy estimator \n",
    "lr_dummy_regr = DummyRegressor(strategy=\"median\")\n",
    "\n",
    "# Train estimator\n",
    "lr_dummy_regr.fit(lr_x_train_ppu, lr_y_train_ppu)\n",
    "\n",
    "# Predict \n",
    "lr_preds = np.exp(lr_dummy_regr.predict(lr_x_test_ppu))\n",
    "\n",
    "# Calculate RMSE\n",
    "mse = mean_squared_error(np.exp(lr_y_test_ppu) * lr_y_test_ru, lr_preds * lr_y_test_ru)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(np.exp(lr_y_test_ppu)* lr_y_test_ru, lr_preds * lr_y_test_ru)\n",
    "\n",
    "# Print results\n",
    "print(\"R-Squared: \", r2_score(np.exp(lr_y_test_ppu) * lr_y_test_ru, lr_preds * lr_y_test_ru))\n",
    "print(\"Root Mean Square Error: \", rmse)\n",
    "print(\"Mean Absolute Error: \", mae)\n",
    "print()\n",
    "```\n",
    "results:\n",
    "\n",
    "\n",
    "```\n",
    "Linear Regression df\n",
    "R-Squared: -0.695\n",
    "Root Mean Square Error: 680,709.220\n",
    "Mean Absolute Error: 475,646.653\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_cI8m6X9LY7A"
   },
   "source": [
    "### Pipeline\n",
    "\n",
    "```\n",
    "numeric_features = ['GROSS SQUARE FEET',\"BLOCK\",\"LOT\", \"NEIGHBORHOOD\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "\n",
    "ohe_features = [\"BUILDING CLASS CATEGORY\", \"BOROUGH\"]\n",
    "ohe_transformer = OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('ohe', ohe_transformer, ohe_features),\n",
    "        ])\n",
    "\n",
    "# Linear regression Estimator\n",
    "lr = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regression', LinearRegression())])\n",
    "\n",
    "# Decision Tree Regressor\n",
    "dtr = Pipeline(steps=[\n",
    "                      ('dtr', DecisionTreeRegressor())])\n",
    "\n",
    "# Extra Tree Regressor\n",
    "etr = Pipeline(steps=[\n",
    "                      ('etr', BaggingRegressor(ExtraTreeRegressor(random_state=42),\n",
    "                                                        random_state = 42))])\n",
    "# Random Forest Regressor\n",
    "rfrr = Pipeline(steps=[\n",
    "                      ('rfregression', RandomForestRegressor(random_state = 42, n_jobs= -1, \n",
    "                                                             n_estimators = 50, \n",
    "                                                             min_samples_leaf = 5))])\n",
    "```\n",
    "\n",
    "We scaled numeric features such as Gross Square Feet, Block, Lot and Neighborhood. We used OneHotEncoder for features with a few unique values such as Building class category and Borough. Additionally, we didn't use the preprocessing step in the pipeline of Random Forest, Decision Tree Regressor and Extra Tree Regressor since data preparation is not required for those models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEnY-y_EUdgG"
   },
   "source": [
    "### Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOmAuuMEUkVr"
   },
   "source": [
    "We implemented two algorithms for cross validation. One uses the sklearn method called cross_validate, and the other one is a modified version of a code taken from Jupyter Notebook Week 2. The former was used for Decision Tree Regressor, Extra Tree Regressor and Random Forest, while the latter was utilized for linear regression due to the process of target engineering in this model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vttBNNyMVtgX"
   },
   "source": [
    "#### Tree Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1NZApjhVx5a"
   },
   "source": [
    "\n",
    "Here is the function we created using the method cross_validate:\n",
    "```\n",
    "def validation(models = [], estimators = [], training = [], cv = 5, train_score = False):\n",
    "    if len(models) != len(estimators):\n",
    "        print(\"Error: model names and estimator must have the same length\")\n",
    "        return\n",
    "\n",
    "    for model, estimator in zip(models, estimators):\n",
    "        scores = cross_validate(estimator, training[0], training[1], cv=5,\n",
    "                            scoring=('r2',\n",
    "                                        'neg_mean_squared_error',\n",
    "                                        'neg_mean_absolute_error'),\n",
    "                            return_train_score=False)\n",
    "        print(model)\n",
    "        print(\"R-Squared: {:,.3f}\".format(np.mean(scores[\"test_r2\"])))\n",
    "        print(\"Root Mean Squared Error: {:,.3f}\".format(np.mean(np.sqrt(-scores[\"test_neg_mean_squared_error\"]))))\n",
    "        print(\"Mean Absolute Error: {:,.3f}\".format(np.mean(-scores[\"test_neg_mean_absolute_error\"])))\n",
    "        print()\n",
    "\n",
    "\n",
    "validation(models = [\"Decision Tree Regressor\", \"Extra Tree Regressor\", \"Random Forest\"], \n",
    "           estimators = [dtr, etr, rfrr],\n",
    "           training = [tr_x_train, np.exp(tr_y_train)])\n",
    "```\n",
    "\n",
    "Results:\n",
    "\n",
    "\n",
    "```\n",
    "Decision Tree Regressor\n",
    "R-Squared: 0.388\n",
    "Root Mean Squared Error: 468,186.233\n",
    "Mean Absolute Error: 263,593.068\n",
    "\n",
    "Extra Tree Regressor\n",
    "R-Squared: 0.634\n",
    "Root Mean Squared Error: 362,126.920\n",
    "Mean Absolute Error: 211,437.049\n",
    "\n",
    "Random Forest\n",
    "R-Squared: 0.649\n",
    "Root Mean Squared Error: 354,855.099\n",
    "Mean Absolute Error: 206,175.793\n",
    "```\n",
    "\n",
    "Cross Validation results are much better than our baseline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB5I8TROVrIT"
   },
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qtjKtA3Wdmm"
   },
   "source": [
    "\n",
    "Below is the modified code taken from Jupyter Notebook Week 2:\n",
    "```\n",
    "# initialize the model\n",
    "kf = KFold(n_splits=10, random_state = 42, shuffle = True)\n",
    "\n",
    "# make a list to store our RMSE \n",
    "lr_RMSEs, lr_R2, lr_MAE = [], [], []\n",
    "\n",
    "# loop over the k folds\n",
    "for train_index, validate_index in kf.split(lr_x_train_ppu):\n",
    "    \n",
    "    # train the model using the training set\n",
    "    lr.fit(lr_x_train_ppu.iloc[train_index,], lr_y_train_ppu.iloc[train_index,])\n",
    "    \n",
    "    # predict on a the validation set\n",
    "    predictions = lr.predict(lr_x_train_ppu.iloc[validate_index,])\n",
    "    predictions = np.exp(predictions) * lr_y_train_ru.iloc[validate_index,]\n",
    "\n",
    "    test_set = lr_y_train.iloc[validate_index]\n",
    "\n",
    "    # Append scores\n",
    "    lr_R2.append(r2_score(test_set, predictions))\n",
    "    \n",
    "    mse = mean_squared_error(test_set,predictions)\n",
    "    lr_RMSEs.append(np.sqrt(mse))\n",
    "    \n",
    "    lr_MAE.append(mean_absolute_error(test_set, predictions))\n",
    "    \n",
    "\n",
    "    \n",
    "# let's look at the output from k fold\n",
    "print(\"Linear Regression: \")\n",
    "print(\"R-Squared: {:,.3f}\".format(np.mean(lr_R2)))\n",
    "print(\"Root Mean Squared Error: {:,.3f}\".format(np.mean(lr_RMSEs)))\n",
    "print(\"Mean Absolute Error: {:,.3f}\".format(np.mean(lr_MAE)))\n",
    "print()\n",
    "```\n",
    "Results:\n",
    "\n",
    "\n",
    "```\n",
    "Linear Regression: \n",
    "R-Squared: 0.252\n",
    "Root Mean Squared Error: 445,688.977\n",
    "Mean Absolute Error: 262,219.829\n",
    "```\n",
    "The results are better than the baseline, but the error is too big to rely on such model. There is no profit margin if the error is 262,000+. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DnfG_B_Lg-d"
   },
   "source": [
    "### Testing\n",
    "After training the different models, we got the results:\n",
    "\n",
    "\n",
    "```\n",
    "Descision Tree:\n",
    "R-Squared: 0.401\n",
    "Root Mean Square Error: 442,844.551\n",
    "Mean Absolute Error: 249,737.346\n",
    "\n",
    "Extra-Tree Regressor:\n",
    "R-Squared: 0.652\n",
    "Root Mean Square Error: 337,775.405\n",
    "Mean Absolute Error: 194,745.277\n",
    "\n",
    "Random Forest:\n",
    "R-Squared: 0.650\n",
    "Root Mean Square Error: 338,407.994\n",
    "Mean Absolute Error: 190,655.493\n",
    "\n",
    "Linear Regression: \n",
    "R-Squared: 0.327\n",
    "Root Mean Square Error: 428,974.407\n",
    "Mean Absolute Error: 259,014.542\n",
    "```\n",
    "\n",
    "Decision Tree and Linear Regression are not preforming well for this problem. Linear Regression is mainly affected by outliers, and therefore we removed them in the data cleaning process. Decision Tree is usually performing bad due to overfitting the training set. Therefore, we also tried Random Forest Regression and Extra Tree Regressor to add randomness to the model and the results are better. They would probably be much better if we had other features such as the number of rooms/bathrooms and/or number of parking lots. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWvVJ2hyMXbI"
   },
   "source": [
    "### Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EEje6REmI-K"
   },
   "source": [
    "#### Decision Tree Regressor\n",
    "RandomizedSearchCV was utilized in order to achieve best results for Decision Tree Regresson. In the code below the parameters are listed:\n",
    "\n",
    "```\n",
    "grid_param = [{\"dtr__max_depth\": randint(low=20, high=60),\n",
    "               \"dtr__min_samples_split\":randint(low=2, high=6),\n",
    "               \"dtr__min_samples_leaf\": randint(low = 2, high = 10)}]\n",
    "```\n",
    "##### Results\n",
    "\n",
    "\n",
    "```\n",
    "Decision Tree Random Search CV\n",
    "R-Squared: 0.583\n",
    "Root Mean Square Error: 369,410.920\n",
    "Mean Absolute Error: 212,902.102\n",
    "\n",
    "Pipeline(steps=[('dtr',\n",
    "                 DecisionTreeRegressor(criterion='mse', max_depth=35,\n",
    "                                       min_samples_leaf=9,\n",
    "                                       min_samples_split=5))])\n",
    "```\n",
    "Results are much better than the previous one. Score was increased from 0.4 to 0.58. We also got a great decrease in RMSE and MAE which is great for our mission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqrwqAWZmUQO"
   },
   "source": [
    "#### Extra Tree Regressor\n",
    "\n",
    "RandomizedSearchCV Parameters:\n",
    "```\n",
    "grid_param = [{\"etr__n_estimators\": randint(low=20, high=60),\n",
    "               \"etr__max_features\":randint(low=1, high=6)}]\n",
    "```\n",
    "##### Results\n",
    "\n",
    "\n",
    "```\n",
    "R-Squared: 0.673\n",
    "Root Mean Square Error: 327,489.884\n",
    "Mean Absolute Error: 186,690.905\n",
    "\n",
    "Pipeline(steps=[('etr',\n",
    "                 BaggingRegressor(base_estimator=ExtraTreeRegressor(criterion='mse',\n",
    "                                                                    random_state=42),\n",
    "                                  max_features=5, n_estimators=40,\n",
    "                                  random_state=42))])\n",
    "```\n",
    "Slightly improved, but not significant. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGAxOoSxmTjm"
   },
   "source": [
    "#### Random Forest\n",
    "Used RandomizedSearchCV with the below parameters:\n",
    "```\n",
    "## Random Search CV\n",
    "grid_param = [{\"rfregression__n_estimators\": randint(low=20, high=100),\n",
    "               \"rfregression__min_samples_leaf\":randint(low=2, high=30)}]\n",
    "\n",
    "```\n",
    "##### Results\n",
    "\n",
    "\n",
    "```\n",
    "Random Forest Random Search CV\n",
    "R-Squared: 0.660\n",
    "Root Mean Square Error: 333,608.167\n",
    "Mean Absolute Error: 187,708.521\n",
    "\n",
    "Pipeline(steps=[('rfregression',\n",
    "                 RandomForestRegressor(criterion='mse', min_samples_leaf=3,\n",
    "                                       n_estimators=75, n_jobs=-1,\n",
    "                                       random_state=42))])\n",
    "```\n",
    "Not much improved than the previous one. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJd9HH8QMddd"
   },
   "source": [
    "---\n",
    "## Conclusions \n",
    "\n",
    "Our dataset mainly contains information about properties location such as Borough, Block, and Lot. It also contains bulding class and tax class. However, there is not information about the properties such as wether its renovated, number of rooms, parking lot, balcony and so on. Those added features could enhance the performance of our models, especially Random Forest and Linear Regression. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF_gS543LszK"
   },
   "source": [
    "---\n",
    "## Future Considerations\n",
    "To enhance our model performance there are other features that might be helpful and are missing in our dataset such as the number of rooms and the number of baths in each property."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSCI 631 - Final Project Report.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
